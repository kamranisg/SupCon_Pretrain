# Pretrain_xrays
Pre-training for localised tasks on Medical Chest-rays

## About

Self-supervised learning has emerged as a powerful technique to understand non-curated datasets (i.e., without labels). In particular, contrastive learning has shown tremendous transfer learning capability in several downstream tasks such as Classification, segmentation and detection. In addition, we exploit the pre-training downstream. In this report, we evaluate three different scenarios involving pre-training and transfer learning to improve the localisation on chest X-rays in terms of bounding boxes.


## Architecture

## Setup

### Enviroment
### Datasets
### Stage-1 (Pre-training)
### Stage-2 (Downstream training and Evalaution)

## Results
